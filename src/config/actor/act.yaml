defaults:
  - base_actor

name: act
flatten_obs: True


ewise_reconstruction_loss_fn: l1_loss
beta_kl: 0.1

inference:
  temporal_ensemble_m: 0.01

# TODO: Fix "hardcodes" by pulling this stuff from the base config into this yaml directly using anchors
ConditionalVAE:
  dim_robot_state: 16
  dim_object_state: 42

  dim_latent: 32
  dim_action: 10

  TransformerEncoder:
    num_layers: 4
    layer_norm: True

    TransformerEncoderLayer:
      dim_model: 512
      num_heads: 8
      dim_feedforward: 3200
      dropout: 0.1
      activation: relu # Can specify any activation function implemented by torch.nn.functional
      normalize_before: False

  ActionTransformer:
    TransformerEncoder:
      num_layers: 6
      layer_norm: True
      TransformerEncoderLayer:
        dim_model: 512
        num_heads: 8
        dim_feedforward: 3200
        dropout: 0.1
        activation: relu
        normalize_before: False

    TransformerDecoder:
      num_layers: 6
      return_intermediate: True
      TransformerDecoderLayer:
        dim_model: 512
        num_heads: 8
        dim_feedforward: 3200
        dropout: 0.1
        activation: relu
        normalize_before: False

action_horizon: ${action_horizon}
pred_horizon: ${pred_horizon}
obs_horizon: 1
predict_past_actions: ${predict_past_actions}









